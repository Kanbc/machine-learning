{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับปัญหา Classification แบบ Logistic Regression ถ้า y = 1 เราก็ต้องการให้ $h_{\\theta}(x) \\approx 1$ ซึ่งหมายความว่า $\\theta^{T}x >> 0$ เหมือนกันกับกรณีที่ $y=0$ ตัว $\\theta^{T}x << 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/58.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "กรณีที่ $y = 1$ จะได้ $J_{1}(\\theta) = -\\log\\frac{1}{1+e^{-z}}$ และที่ $y = 0$ จะได้ $J_{0}(\\theta) = -\\log(1-\\frac{1}{1+e^{-z}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/59.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากรูปข้างบนจะเห็น ถ้าเราประมาณกราฟ $-\\log\\frac{1}{1+e^{-z}}$ กับ $-\\log(1-\\frac{1}{1+e^{-z}})$ ด้วยกราฟที่คล้ายๆกัน คือ $\\text{Cost}_{1}(z)$ กับ $\\text{Cost}_{0}(z)$ เมื่อ $y = 1$ และ $0$ ตามลำดับ จะได้ว่า"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/60.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่า Cost Function ของ SVM แม่งคล้ายๆ LR ที่แทน  $-\\log\\frac{1}{1+e^{-z}}$ กับ $-\\log(1-\\frac{1}{1+e^{-z}})$ ด้วย $\\text{Cost}_{1}(z)$ กับ $\\text{Cost}_{0}(z)$ ตามลำดับ เอา $\\frac{1}{m}$ ออก แล้วก็ย้าย $\\lambda$ ที่อยู่พจน์ข้างหลัง(Regularize) ไปไว้ข้างหน้าแทน (ให้ผลเหมือนกันแหละ เช่น ถ้าเราอยากให้นำหนักพจน์ข้างหลังเยอะๆ ก็เอา $C$ ค่าน้อยๆคูณพจน์ข้างหน้าแทน เป็นต้น)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/61.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Margin Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/62.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับ SVM เราจะทายว่า $y = 1$ เมื่อ $\\theta^{T}x >= 1$ หรือ $y = 0$ เมื่อ $\\theta^{T}x < -1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/63.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ซึ่งหมายความว่าค่า เส้น decision boundary จะหนาขึ้นปะ โดยจะรวม $\\theta^{T}x$ ที่อยู่ระหว่าง $-1$ ถึง $1$ ด้วย มองเป็น space ว่างๆ ซึ่งเรียกว่า margin ดังรูปข้างล่าง (SVM จะเลือก decision boundary ที่มี margin หนาที่สุด)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/64.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทีนี้จาก Cost Function ถ้า C มีค่ามากๆ เวลา fit model กับ data มันจะ sensitive กับข้อมูลมาก (ตรงข้ามกับกรณี $\\lambda$ มากๆ) อย่างในรูปข้างล่าง ที่ C มากๆ แม้มี X ไม่กี่ตัวอยู่ในกลุ่ม O เวลา fit เส้น decision boundary มันก็พยายามจะเบ้เข้าไปกวาดให้มาอยู่ฝั่งเดียวกันให้หมด"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/65.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics Behind Large Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/66.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทบทวน สมมติมี vector 2 อันคือ $u$ กับ $v$ จะหาขนาดของ $v$ บน $u$ ก็ต้อง $u\\cdot v$ ซึ่ง $u \\cdot v = u^Tv$ (อ่าน $u\\cdot v$ อ่านว่าทำ inner product กัน) หรือเท่ากับ  $p \\cdot ||u||$ เมื่อ $||u||$ คือขนาดของเวกเตอร์ $u$ โดย $p$ จะมีค่าบวกเมื่อมุมระหว่าง $u$ กับ $v$ น้่อยกว่า 90 องศา ถ้า**มากกว่าจะเป็นลบ** \n",
    "\n",
    "We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เพื่อให้ง่ายต่อการ plot เริ่มจากกรณีง่ายๆก่อน (Simplication) ที่ $C = 0,\\theta_0 = 0$ (decision boundary จะตัดจุด origin) และมี feature แค่ 2 ตัว คือ $x_1,x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/67.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถ้า The optimization objective of the SVM คือ $\\text{min}_{\\theta}\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j$ แล้วเรามี features แค่ 2 ตัวจะได้ $\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j = \\frac{1}{2}(\\theta^2_1+\\theta^2_2)$ ดังรูปข้างบน เท่ากับ $\\frac{1}{2}(||\\theta||)^2$ ซึ่ง $||\\theta||$ ก็คือความยาวของเวกเตอร์ $\\theta$ (the norm of the vector theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p^{(i)}$ คือ $x^{(i)}$ ที่ projection ลง $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/68.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the SVM can make the norm of the parameters theta much smaller. So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เขียน $\\theta^Tx^{(i)}$ ในรูปของ $p^{(i)}\\cdot||\\theta||$ จะได้"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p^{(i)}\\cdot||\\theta|| >= 1 $ ถ้า $y^{(i)}$ = 1\n",
    "- $p^{(i)}\\cdot||\\theta|| <= -1$ ถ้า $y^{(i)}$ = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ต่อไปพิจารณา training example แล้วดูว่า decision boundary ที่ SVM จะเลือกจะเป็นยังไง จากรูปข้างบน\n",
    "- ฝั่งซ้ายเป็นแบบที่ SVM จะไม่เลือกเป็น decision boundary เพราะไม่ใช่ตัวเลือกที่ดี เพราะว่า margin มันเล็ก เทียบกับฝั่งขวา margin ใหญ่กว่า เป็น decision boundary ที่ดีกว่าชัดเจน (margin = min($p$))\n",
    "- Simplication ที่ $\\theta_0 = 0$ (decision boundary จะตัดจุด origin) ถ้าไม่เท่ากับ 0 มันจะไม่ตัดจุด (0,0)\n",
    "- จาก optimization objective ยิ่งค่า $p^{(i)}\\cdot||\\theta|| >= 1 $ มากๆ (เช่น $p^{(i)}\\cdot||\\theta|| = 90$) หรือ $p^{(i)}\\cdot||\\theta|| <= -1$ มากๆยิ่งดี (เช่น $p^{(i)}\\cdot||\\theta|| = -10$) มันคือการแบ่งระหว่าง 2 กลุ่มได้ชัดขึ้น \n",
    "- ซึ่งการจะเป็นอย่างนั้นได้เนี่ย หาก $p$ มันเล็กมาก $\\theta$ ต้องใหญ่ขึ้น เพื่อให้ผลคูณมันออกมาเยอะ กลับกัน หาก $p$ ใหญ่แล้ว $\\theta$ ก็เล็กได้ ผลคูณก็จะออกมาเยอะเหมือนกัน\n",
    "- **decision boundary ที่ดี $||\\theta||$ จะน้อย** (margin ต้องเยอะๆ) ซึ่งตรงกับปัญหา optimization ของ SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's this large margin, there's this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on that's **the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective**. And this is why this machine ends up with enlarge margin classifiers because it trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แม้ตัวอย่างเราจะคิดที่ $C = 0$ อย่างไรก็ตาม ในกรณีที่ C มีค่ามากๆ ก็มีวิธีคิดแบบนี้เหมือนกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you know, when theta is not equal to 0 this **support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples**. So that explains how this support vector machine is a large margin classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หนึ่งในวิธีการที่จะทำให้ decision boundary เป็นแบบ non-linear แล้วแก้ปัญหาที่ซับซ้อนขึ้นได้คือ เพิ่ม feature ที่เป็น polynomial เข้าไป\n",
    "\n",
    "โดย kernels คือการสร้าง features ใหม่ เพื่อใช้กับ SVM\n",
    "\n",
    "จากรูปข้างล่างเขียนพจน์ $x$ ให้อยู่ในรูปของ $f$ (feature) แทน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/69.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับ kernel มันเริ่มจากการกำหนด landmark ($l^{(1)},l^{(2)},l^{(3)}$) ขึ้นมา ซึ่งก็คือจุด 3 ค่าบนระนาบ $x1-x2$ นั่นเอง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/70.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากรูปข้างบนจะเห็นว่าถ้าเรามีจุด landmark ($l^{(1)},l^{(2)},l^{(3)}$) 3 จุด ก็จะมี $(f_1,f_2,f_3)$ 3 feature เหมือนกัน ($f_i$ ก็คือ feature ใหม่ของเราที่ map มาจาก $x$ และ $l^{(i)}$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "มาลองแทนค่า $x$ เพื่อหา $f$ กัน ตามรูปข้างล่างเลย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/71.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สังเกตุว่าถ้า $x$ มีค่าใกล้ $l^{(1)}$ มากๆ $||x-l^{(1)}||$ จะมีค่าใกล้ 0 ทำให้ $f_1 \\approx 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ส่วนถ้า $x$ อยู่ห่างจาก $l^{(1)}$ จะทำให้ $||x-l^{(1)}||$ มีค่ามาก แล้ว $f_1 \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดูแบบ visualize สักหน่อย เมื่อมี landmark อยู่ที่ $(x_1,x_2) = (3,5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/72.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่าที่ (3,5) $f_1$ จะมีค่าสูงสุด และจุดที่ห่างออกไปจะมีค่าต่ำลงเรื่อยๆ ซึ่งค่า $\\sigma^2$ มีผลต่ออัตตราการเปลี่ยนแปลงของ $f_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "มาลองพิจารณาแบบรวมๆ กรณีมี landmark 3 จุด **(โดยสมมติ ว่าเลือก landmark และ $\\theta$ แล้ว)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/73.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากรูปข้างบน เมื่อ $\\theta_0=-0.5$,$\\theta_1=1$,$\\theta_2=1$,$\\theta_0=0$ แล้ว\n",
    "\n",
    "### เราจะทายว่าเป็น 1 เมื่อ\n",
    "$$\\theta_0+\\theta_1f_1+\\theta_2f_2+\\theta_3f_3 >= 0$$\n",
    "\n",
    "ซึ่งเมื่อลองแทนๆค่าดูแล้วจะเห็นว่าที่บริเวณใกล้ๆ $l^{(1)},l^{(2)}$ เท่านั้นที่จะเป็น 1 นอกนั้น 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ลองมาใช้จริงกับชุดข้อมูล $x_1,x_2,\\cdots,x_m$ เลือก landmark เป็นจุดเดียวกับจุดข้อมูลทุกจุดที่เรามี"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice this is **how the landmarks are chosen** which is that given the machine learning problem. We have some data set of some some positive and negative examples. \n",
    "\n",
    "> We're just going to **put landmarks as exactly the same locations as the training examples**. \n",
    "\n",
    "So if I have one training example if that is x1, well then I'm going to choose this is my first landmark to be at xactly the same location as my first training example. \n",
    "\n",
    "And if I have a different training example x2. Well we're going to set the second landmark to be the location of my second training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/74.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the figure on the right, I used red and blue dots just as illustration, the color of this figure, the color of the dots on the figure on the right is not significant. \n",
    "\n",
    "But what I'm going to end up with using this method is I'm going to end up with m landmarks of l1, l2 \n",
    "down to l(m) if I have m training examples with one landmark per location of my per location of each of my training examples. And this is nice because it is saying that my features are basically going to measure how close an example is to one of the things I saw in my training set. \n",
    "\n",
    "So, just to write this outline a little more concretely, given m training examples, I'm going to choose the the location of my landmarks to be exactly near the locations of my m training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> จากในรูป จะเห็นว่า $f_1,f_2,f_3,\\cdots,f_m$ มีค่าเท่ากับจำนวน landmarks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### สรุปคือ\n",
    "- เลือก landmark เป็นจุดเดียวกับจุดข้อมูล ดังนั้นจะมี $l^{(1)},l^{(2)},\\cdots,l^{(m)}$\n",
    "- สำหรับจุดข้อมูล $x^{(i)}$ หนึ่งจุดจะมี $f^{(i)} \\in \\mathbb{R}^{mx1}$ แต่ถ้าเพิ่ม $f^{(i)}_0$ ด้วยก็จะได้ $f^{(i)} \\in \\mathbb{R}^{(m+1)x1}$\n",
    "- หมายความว่า สำหรับจุดข้อมูล $(x^{(i)},y^{(i)})$ ต้อง map ไปเป็น vector $f^{(i)}$ ถ้ามีจุด m จุดก็จะมีเวกเตอร์ $f \\in \\mathbb{R}^{mx1}$ m ตัว\n",
    "- ณ จุดที่ $x^{(i)} = l^{(i)}$ similarity($x^{(i)},l^{(i)}$) จะเท่ากับ 1\n",
    "- กรณีมีหลายฟีเจอร์ $x_1,x_2,\\cdots,x_n$ ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/75.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ถ้ารู้ $\\theta$ แล้ว จะหาค่า prediction ก็ตามรูปข้างล่างเลย ถ้า $\\theta^Tf >= 0$ ก็ให้ทายว่าเป็น 1 ไป\n",
    "- เมื่อจำนวน landmarks = จำนวนจุด training set (m) จะได้ว่าจำนวน parameter ของ model คือ m+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### วิธีการหาค่า $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- จากเดิมที่ minimized Cost Function ของ $x^{(i)}$ ให้เปลี่ยน $x^{(i)}$ เป็น $f^{(i)}$ แทน\n",
    "- Solve สมการในรูปข้างล่างก็จะได้ $\\theta$\n",
    "- จำนวน $\\theta$ คือ n = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/76.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ด้วยที่ n (จำนวน features) เท่ากับ m (จำนวนข้อมูล) ถ้าหากข้อมูลมีจำนวนมากๆเป็นหมื่นๆ จะทำให้ $\\theta^T\\theta$ ใหญ่มาก อาจคูณลำบาก อาจต้องแปลงเป็น $\\theta^TM\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance & Bias when using a SVM\n",
    "When using an SVM, one of the things you need to choose is the parameter C which was in the optimization objective, and you recall that C played a role similar to $\\frac{1}{\\lambda}$, where $\\lambda$ was the regularization parameter we had for logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/77.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ถ้าเราให้ C ใหญ่ๆ model จะ overfiting (high variance) เพราะเราให้ความสำคัญกับพจน์ error มาก แต่ให้ความสำคัญกับพจน์ regularization น้อย\n",
    "- กลับกัน ถ้าให้ C น้อยๆ พจน์ regularization จะใหญ่ขึ้น model จะ underfittion (High bias)\n",
    "- **C คือหนึ่งใน parameter ที่เราต้องเลือก**\n",
    "- $\\sigma^2$ ก็เป็นพารามิเตอร์อีกตัวที่เราต้องเลือกเหมือนกัน ยิ่ง $\\sigma^2$ มีค่ามาก model จะลู่ไปทาง underfitting แต่ถ้าเล็กจะทำให้ overfitting\n",
    "\n",
    "$$ \\text{Gaussian kernel} = \\text{exp}(\\frac{||x-l^{(i)}||^2}{2\\sigma^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using An SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM แม่งเป็นอัลกอริทึมยาก ไม่แนะนำให้เขียนเอง มักใช้ software ช่วย\n",
    "\n",
    "ถึงจะใช้ software อื่นทำแต่ก็มีบางอย่างที่ต้องเลือก ดังนี้\n",
    "- ค่า C\n",
    "- เลือก kernel (similarity function) เช่น linear kernel (สำหรับจะแบ่งแบบ linear), Gaussian kernel (ถ้าเลือก GS ต้องเลือก $\\sigma^2$ ด้วย),..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/78.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตัวอย่างการใช้งานฟังก์ชั่น Gaussian kernel (อันนี้ของ MATLAB)\n",
    "\n",
    "### ต้องทำ feature scaling ก่อนทำ Gaussian kernel ด้วยนะ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/79.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เลือกใช้ kernel มีให้เลือกหลายแบบ ส่วนมากมี input แค่ x กับ l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/80.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การทำ multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/81.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีไหนควรใช้ Logistic Regression , NN หรือ SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/82.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================== CODE ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
