{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimization Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับปัญหา Classification แบบ Logistic Regression ถ้า y = 1 เราก็ต้องการให้ $h_{\\theta}(x) \\approx 1$ ซึ่งหมายความว่า $\\theta^{T}x >> 0$ เหมือนกันกับกรณีที่ $y=0$ ตัว $\\theta^{T}x << 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/58.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "กรณีที่ $y = 1$ จะได้ $J_{1}(\\theta) = -\\log\\frac{1}{1+e^{-z}}$ และที่ $y = 0$ จะได้ $J_{0}(\\theta) = -\\log(1-\\frac{1}{1+e^{-z}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/59.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากรูปข้างบนจะเห็น ถ้าเราประมาณกราฟ $-\\log\\frac{1}{1+e^{-z}}$ กับ $-\\log(1-\\frac{1}{1+e^{-z}})$ ด้วยกราฟที่คล้ายๆกัน คือ $\\text{Cost}_{1}(z)$ กับ $\\text{Cost}_{0}(z)$ เมื่อ $y = 1$ และ $0$ ตามลำดับ จะได้ว่า"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/60.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่า Cost Function ของ SVM แม่งคล้ายๆ LR ที่แทน  $-\\log\\frac{1}{1+e^{-z}}$ กับ $-\\log(1-\\frac{1}{1+e^{-z}})$ ด้วย $\\text{Cost}_{1}(z)$ กับ $\\text{Cost}_{0}(z)$ ตามลำดับ เอา $\\frac{1}{m}$ ออก แล้วก็ย้าย $\\lambda$ ที่อยู่พจน์ข้างหลัง(Regularize) ไปไว้ข้างหน้าแทน (ให้ผลเหมือนกันแหละ เช่น ถ้าเราอยากให้นำหนักพจน์ข้างหลังเยอะๆ ก็เอา $C$ ค่าน้อยๆคูณพจน์ข้างหน้าแทน เป็นต้น)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/61.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Margin Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/62.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สำหรับ SVM เราจะทายว่า $y = 1$ เมื่อ $\\theta^{T}x >= 1$ หรือ $y = 0$ เมื่อ $\\theta^{T}x < -1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/63.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ซึ่งหมายความว่าค่า เส้น decision boundary จะหนาขึ้นปะ โดยจะรวม $\\theta^{T}x$ ที่อยู่ระหว่าง $-1$ ถึง $1$ ด้วย มองเป็น space ว่างๆ ซึ่งเรียกว่า margin ดังรูปข้างล่าง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/64.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทีนี้จาก Cost Function ถ้า C มีค่ามากๆ เวลา fit model กับ data มันจะ sensitive กับข้อมูลมาก (ตรงข้ามกับกรณี $\\lambda$ มากๆ) อย่างในรูปข้างล่าง ที่ C มากๆ แม้มี X ไม่กี่ตัวอยู่ในกลุ่ม O เวลา fit เส้น decision boundary มันก็พยายามจะเบ้เข้าไปกวาดให้มาอยู่ฝั่งเดียวกันให้หมด"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/65.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics Behind Large Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/66.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทบทวน สมมติมี vector 2 อันคือ $u$ กับ $v$ จะหาขนาดของ $v$ บน $u$ ก็ต้อง $u\\cdot v$ ซึ่ง $u \\cdot v = u^Tv$ (อ่าน $u\\cdot v$ อ่านว่าทำ inner product กัน) หรือเท่ากับ  $p \\cdot ||u||$ เมื่อ $||u||$ คือขนาดของเวกเตอร์ $u$ โดย $p$ จะมีค่าบวกเมื่อมุมระหว่าง $u$ กับ $v$ น้่อยกว่า 90 องศา ถ้า**มากกว่าจะเป็นลบ** \n",
    "\n",
    "We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เพื่อให้ง่ายต่อการ plot เริ่มจากกรณีง่ายๆก่อน (Simplication) ที่ $C = 0,\\theta_0 = 0$ (decision boundary จะตัดจุด origin) และมี feature แค่ 2 ตัว คือ $x_1,x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/67.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถ้า The optimization objective of the SVM คือ $\\text{min}_{\\theta}\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j$ แล้วเรามี features แค่ 2 ตัวจะได้ $\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j = \\frac{1}{2}(\\theta^2_1+\\theta^2_2)$ ดังรูปข้างบน เท่ากับ $\\frac{1}{2}(||\\theta||)^2$ ซึ่ง $||\\theta||$ ก็คือความยาวของเวกเตอร์ $\\theta$ (the norm of the vector theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p^{(i)}$ คือ $x^{(i)}$ ที่ projection ลง $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/68.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the SVM can make the norm of the parameters theta much smaller. So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เขียน $\\theta^Tx^{(i)}$ ในรูปของ $p^{(i)}\\cdot||\\theta||$ จะได้"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p^{(i)}\\cdot||\\theta|| >= 1 $ ถ้า $y^{(i)}$ = 1\n",
    "- $p^{(i)}\\cdot||\\theta|| <= -1$ ถ้า $y^{(i)}$ = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ต่อไปพิจารณา training example แล้วดูว่า decision boundary ที่ SVM จะเลือกจะเป็นยังไง จากรูปข้างบน\n",
    "- ฝั่งซ้ายเป็นแบบที่ SVM จะไม่เลือกเป็น decision boundary เพราะไม่ใช่ตัวเลือกที่ดี เพราะว่า margin มันเล็ก เทียบกับฝั่งขวา margin ใหญ่กว่า เป็น decision boundary ที่ดีกว่าชัดเจน (margin = min($p$))\n",
    "- Simplication ที่ $\\theta_0 = 0$ (decision boundary จะตัดจุด origin) ถ้าไม่เท่ากับ 0 มันจะไม่ตัดจุด (0,0)\n",
    "- จาก optimization objective ยิ่งค่า $p^{(i)}\\cdot||\\theta|| >= 1 $ มากๆ (เช่น $p^{(i)}\\cdot||\\theta|| = 90$) หรือ $p^{(i)}\\cdot||\\theta|| <= -1$ มากๆยิ่งดี (เช่น $p^{(i)}\\cdot||\\theta|| = -10$) มันคือการแบ่งระหว่าง 2 กลุ่มได้ชัดขึ้น \n",
    "- ซึ่งการจะเป็นอย่างนั้นได้เนี่ย หาก $p$ มันเล็กมาก $\\theta$ ต้องใหญ่ขึ้น เพื่อให้ผลคูณมันออกมาเยอะ กลับกัน หาก $p$ ใหญ่แล้ว $\\theta$ ก็เล็กได้ ผลคูณก็จะออกมาเยอะเหมือนกัน\n",
    "- **decision boundary ที่ดี $||\\theta||$ จะน้อย** (margin ต้องเยอะๆ) ซึ่งตรงกับปัญหา optimization ของ SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's this large margin, there's this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on that's **the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective**. And this is why this machine ends up with enlarge margin classifiers because it trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แม้ตัวอย่างเราจะคิดที่ $C = 0$ อย่างไรก็ตาม ในกรณีที่ C มีค่ามากๆ ก็มีวิธีคิดแบบนี้เหมือนกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you know, when theta is not equal to 0 this **support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples**. So that explains how this support vector machine is a large margin classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
